{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start State:\n",
      " [[ 9 15 12  3]\n",
      " [ 7 14 10  2]\n",
      " [ 0  5  8  1]\n",
      " [11  4  6 13]]\n",
      "Solution found in 62 moves!\n",
      "Final state after solving:\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [13 14 15  0]]\n",
      "Total actions evaluated: 66141419\n",
      "Efficiency (solution moves / actions evaluated): 0.000001\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "from collections import namedtuple\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from random import choice\n",
    "\n",
    "\n",
    "# Configuration\n",
    "PUZZLE_DIM = 4\n",
    "RANDOMIZE_STEPS = 10000\n",
    "action = namedtuple('Action', ['pos1', 'pos2'])\n",
    "\n",
    "\n",
    "# Define goal state and helpers\n",
    "GOAL_STATE = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "GOAL_POSITIONS = {value: divmod(idx, PUZZLE_DIM) for idx, value in enumerate(GOAL_STATE.flatten())}\n",
    "\n",
    "def available_actions(state: np.ndarray, blank_pos: tuple[int, int]) -> list['Action']:\n",
    "    x, y = blank_pos\n",
    "    actions = []\n",
    "    if x > 0:\n",
    "        actions.append(action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "\n",
    "# Perform an action\n",
    "def do_action(state: np.ndarray, action: 'Action') -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state\n",
    "\n",
    "def state_to_bytes(state: np.ndarray) -> bytes:\n",
    "    \"\"\"Convert state to a bytes representation.\"\"\"\n",
    "    return state.flatten().tobytes()\n",
    "\n",
    "def bytes_to_state(state_bytes: bytes) -> np.ndarray:\n",
    "    \"\"\"Convert bytes representation back to state.\"\"\"\n",
    "    return np.frombuffer(state_bytes, dtype=int).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "\n",
    "\n",
    "\n",
    "def weighted_manhattan_distance(state: np.ndarray, weights=(1.5, 1.0)) -> float:\n",
    "    \"\"\"Manhattan distance with weighted costs.\"\"\"\n",
    "    distance = 0\n",
    "    for idx, value in enumerate(state.flatten()):\n",
    "        if value != 0:\n",
    "            current_pos = divmod(idx, PUZZLE_DIM)\n",
    "            goal_pos = GOAL_POSITIONS[value]\n",
    "            tile_weight = weights[1] if value < PUZZLE_DIM**2 // 2 else weights[0]\n",
    "            distance += tile_weight * (abs(current_pos[0] - goal_pos[0]) + abs(current_pos[1] - goal_pos[1]))\n",
    "    return distance\n",
    "\n",
    "# A* Solver\n",
    "def a_star_solver(start_state: np.ndarray):\n",
    "    start_bytes = state_to_bytes(start_state)\n",
    "    blank_idx = start_state.flatten().tolist().index(0)\n",
    "    \n",
    "    frontier = [(weighted_manhattan_distance(start_state), 0, start_bytes, blank_idx, None)]\n",
    "    heapq.heapify(frontier)\n",
    "    visited = {}\n",
    "    parents = {}\n",
    "    actions_evaluated = 0  # Counter for total actions evaluated\n",
    "\n",
    "    while frontier:\n",
    "        f_score, g_score, current_state_bytes, blank_idx, parent = heapq.heappop(frontier)\n",
    "        \n",
    "        if current_state_bytes in visited and visited[current_state_bytes] <= g_score:\n",
    "            continue\n",
    "        visited[current_state_bytes] = g_score\n",
    "        parents[current_state_bytes] = parent\n",
    "        \n",
    "        current_state_np = bytes_to_state(current_state_bytes)\n",
    "        blank_pos = divmod(blank_idx, PUZZLE_DIM)\n",
    "        \n",
    "        if current_state_np.tobytes() == GOAL_STATE.tobytes():\n",
    "            return reconstruct_path(parents, current_state_bytes), actions_evaluated\n",
    "        \n",
    "        for action in available_actions(current_state_np, blank_pos):\n",
    "            new_state_np = do_action(current_state_np, action)\n",
    "            new_state_bytes = state_to_bytes(new_state_np)\n",
    "            new_blank_idx = action.pos2[0] * PUZZLE_DIM + action.pos2[1]\n",
    "            \n",
    "            actions_evaluated += 1  # Increment counter for every action considered\n",
    "\n",
    "            if new_state_bytes not in visited or visited[new_state_bytes] > g_score + 1:\n",
    "                heapq.heappush(frontier, (\n",
    "                    g_score + 1 + weighted_manhattan_distance(new_state_np),\n",
    "                    g_score + 1,\n",
    "                    new_state_bytes,\n",
    "                    new_blank_idx,\n",
    "                    current_state_bytes\n",
    "                ))\n",
    "    return None, actions_evaluated\n",
    "\n",
    "def reconstruct_path(parents, state_bytes):\n",
    "    \"\"\"Reconstruct the path from start to goal.\"\"\"\n",
    "    path = []\n",
    "    while state_bytes is not None:\n",
    "        path.append(state_bytes)\n",
    "        state_bytes = parents[state_bytes]\n",
    "    return path[::-1]  # Reverse to get the path from start to goal\n",
    "\n",
    "\n",
    "# Randomization\n",
    "# Solve and report results\n",
    "print(\"Start State:\\n\", state)\n",
    "solution_path, total_actions_evaluated = a_star_solver(state)\n",
    "\n",
    "if solution_path:\n",
    "    print(f\"Solution found in {len(solution_path) - 1} moves!\")\n",
    "    print(\"Final state after solving:\")\n",
    "    final_state = bytes_to_state(solution_path[-1])\n",
    "    print(final_state)  # Print the final solved state\n",
    "    # Calculate and print efficiency\n",
    "    efficiency = (len(solution_path) - 1) / total_actions_evaluated if total_actions_evaluated > 0 else 0\n",
    "    print(f\"Total actions evaluated: {total_actions_evaluated}\")\n",
    "    print(f\"Efficiency (solution moves / actions evaluated): {efficiency:.6f}\")\n",
    "else:\n",
    "    print(\"No solution exists.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CI-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
